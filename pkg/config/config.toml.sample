# Ergs Configuration File
# Edit this file to configure your datasources

storage_dir = '/home/user/.local/share/ergs'

# Real-time event streaming (optional)
# When running 'ergs serve' (warehouse) and 'ergs web' (API) as separate processes,
# set this to a Unix domain socket path to enable live firehose streaming over
# the socket. Leave empty or comment out to disable.
# Recommended runtime location (systemd): /run/ergs/bridge.sock
# Example:
# event_socket_path = '/run/ergs/bridge.sock'
# NOTE: The web process needs read access to the socket file; adjust permissions accordingly.
# If unspecified, real-time WebSocket firehose will not stream live updates.
#event_socket_path = ''


# Importer API configuration
# Used by both the importer API server (ergs importer) and importer datasource
[importer]
# API key for authentication. If not set, a random key will be generated when starting the importer server.
# Copy the generated key here to persist it across restarts.
api_key = ''
# Host to bind the importer API server to (default: localhost)
# host = 'localhost'
# Port for the importer API server (default: 9090)
# port = '9090'

[datasources]

# GitHub - Fetch your GitHub activity, starred repos, and interactions
# [datasources.github]
# type = 'github'
#  interval = '30m0s'  # Optional: custom fetch interval
# [datasources.github.config]
# token = ''  # Required: Your GitHub personal access token
            # Without a token you'll be rate-limited.
# Uncomment and configure additional datasources as needed:

# # Codeberg - Fetch Codeberg activity and repository events
# [datasources.codeberg]
# type = 'codeberg'
# # interval = '45m0s'
# [datasources.codeberg.config]
# token = ''  # Required: Your Codeberg access token

# # Firefox - Extract browsing history from Firefox's places.sqlite
# [datasources.firefox]
# type = 'firefox'
# # interval = '30m0s'  # Optional: custom fetch interval (default: 30m0s)
# # interval = '0s'     # Use 0 to disable automatic fetching (schema-only, useful with importer)
# [datasources.firefox.config]
# database_path = '/path/to/firefox/profile/places.sqlite'  # Required: Path to places.sqlite

# # Chromium - Extract browsing history from Chromium's History database
# [datasources.chromium]
# type = 'chromium'
# # interval = '30m0s'  # Optional: custom fetch interval (default: 30m0s)
# # interval = '0s'     # Use 0 to disable automatic fetching (schema-only, useful with importer)
# [datasources.chromium.config]
# database_path = '/home/user/.config/chromium/Default/History'  # Required: Path to History database

# # Zed Threads - Chat history from Zed editor
# [datasources.zed]
# type = 'zedthreads'
# # interval = '1h0m0s'
# [datasources.zed.config]
# # Uses default path: ~/.local/share/zed/threads/threads.db

# # Gas Stations - Local gas station prices and information
# [datasources.gas_stations]
# type = 'gasstations'
# # interval = '2h0m0s'
# [datasources.gas_stations.config]
# latitude = 41.4847   # Required: Your latitude
# longitude = 2.3199   # Required: Your longitude
# radius = 10000       # Required: Search radius in meters

# # Home Assistant - Capture events via WebSocket API
# [datasources.home]
# type = 'homeassistant'
# # interval = '15s'  # Short interval sampling approximates near-real-time (recommended)
# [datasources.home.config]
# url = 'ws://homeassistant.local:8123/api/websocket'  # Or wss://your.domain/api/websocket
# token = ''                # Required: Long-lived access token (Profile -> Long-Lived Tokens)
# max_events = 100          # Optional: max events per fetch (default 100, max 1000)
# idle_timeout = '5m'       # Optional: end fetch early if no events arrive for this period (default 5m)
# # event_types = ['state_changed','call_service']  # Optional: filter specific event types; empty = all
#
# # HackerNews - Fetch stories, comments, and other items from Hacker News
[datasources.hackernews]
type = 'hackernews'
# # interval = '1h0m0s'
[datasources.hackernews.config]
fetch_top = true        # Fetch top stories (default: true)
fetch_new = false       # Fetch new stories (default: false)
fetch_ask = false       # Fetch Ask HN stories (default: false)
fetch_show = false      # Fetch Show HN stories (default: false)
fetch_jobs = false      # Fetch job postings (default: false)
max_items = 100         # Maximum items to fetch (default: 100, max: 500)
fetch_comments = false  # Also fetch top-level comments (default: false)

# # RSS - Fetch articles from RSS/Atom feeds
[datasources.rss]
type = 'rss'
# interval = '2h0m0s'
[datasources.rss.config]
urls = [
    'https://feeds.arstechnica.com/arstechnica/index',
    'https://www.phoronix.com/rss.php',
    'https://www.theguardian.com/world/rss'
]
max_items = 50  # Maximum items to fetch across all feeds (default: 50, max: 200)

# # Timestamp - Simple timestamp logging (useful for testing)
# [datasources.timestamp]
# type = 'timestamp'
# # interval = '5m0s'
# [datasources.timestamp.config]
# interval_seconds = 60  # Generate timestamp every N seconds during fetch

# # RTVE - Fetch episodes from RTVE (Spanish TV) shows
# [datasources.rtve]
# type = 'rtve'

# # Datadis - Fetch electricity consumption data from Datadis (Spanish electricity data platform)
# [datasources.datadis]
# type = 'datadis'
# # interval = '24h0m0s'  # Fetch once per day (default: 24h)
# [datasources.datadis.config]
# username = ''  # Required: Your Datadis username
# password = ''  # Required: Your Datadis password

# # Importer - Generic datasource for importing blocks from external sources
# # This allows external tools to push blocks via HTTP API
# # Only ONE importer datasource is needed - it routes blocks to their target datasources
# [datasources.importer]
# type = 'importer'
# # interval = '5m0s'  # How often to fetch blocks from the importer API
# [datasources.importer.config]
# api_url = 'http://localhost:9090'  # URL of the importer API server (default: http://localhost:9090)
# api_key = ''  # API key for authentication. Uses [importer] api_key if not set here.
#
# # To use the importer:
# # 1. Configure an API key in [importer] section above (or let it auto-generate)
# # 2. Start the importer API server: ergs importer --port 9090
# # 3. Start the warehouse: ergs serve
# # 4. External tools POST blocks to: http://localhost:9090/api/import/blocks
# #    (Include Authorization: Bearer <key> header)
# # 5. Each block must include a "datasource" field specifying where it should be stored
# # 6. The importer datasource fetches blocks via HTTP and routes them automatically
# #
# # Example block format:
# # {
# #   "blocks": [
# #     {
# #       "id": "unique-id",
# #       "text": "searchable text",
# #       "created_at": "2024-01-15T10:30:00Z",
# #       "type": "github",
# #       "datasource": "github",  // Routes to [datasources.github]
# #       "metadata": {...}
# #     }
# #   ]
# # }
# #
# # See docs/datasources/importer.md for detailed documentation
# # interval = '1h0m0s'
# [datasources.rtve.config]
# show_id = 'telediario-1'  # Required: RTVE show ID (e.g., 'telediario-1', 'telediario-2', 'informe-semanal')
# max_episodes = 10         # Maximum number of latest episodes to fetch (default: 10, max: 100)
